{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Machine Learning <br> Workshop\n",
    "\n",
    "## Zigfried Hampel-Arias\n",
    "\n",
    "### IIHE -- Brussels, BE\n",
    "20 April, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GitHub repo with materials:\n",
    "https://github.com/zhampel/intro-ml-iihe <br>\n",
    "\n",
    "### Slides:\n",
    "https://zhampel.github.io/intro-ml-iihe\n",
    "\n",
    "### Contact:\n",
    "E-mail: [zhampel@wipac.wisc.edu](mailto:zhampel@wipac.wisc.edu)\n",
    "\n",
    "<a href=\"https://zhampel.github.io/\">\n",
    "<img src=\"images/octocat.png\" alt=\"Go My GitHub\" width=\"60\" height=\"60\" border=\"0\"> </a>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/zhampel-arias/\">\n",
    "<img src=\"images/LinkedIn-InBug-2C.png\" alt=\"Go to my LinkedIn\" width=\"60\" height=\"60\" border=\"0\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Program for Today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Intro to Machine Learning\n",
    "\n",
    "- Types of Learning\n",
    "- Machine Learning Basics\n",
    "- Building More Complex Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Keras Workshop\n",
    "\n",
    "- Simple Neural Nets\n",
    "- Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classical Programming\n",
    "\n",
    "- Set of rules to accomplish a task\n",
    "- 'if' this then 'do that'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def spam_filter(email):\n",
    "    \"\"\"Function that labels an email as 'spam' or 'not spam'\n",
    "    \"\"\"\n",
    "    if 'Act now!' in email.contents:\n",
    "        label = 'spam'\n",
    "    elif 'hotmail.com' in email.sender:\n",
    "        label = 'spam'\n",
    "    elif email.contents.count('$') > 20:\n",
    "        label = 'spam'\n",
    "    else:\n",
    "        label = 'not spam'\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "- \"Field of study that gives computers the ability to learn without being explicitly programmed\" &mdash; Arthur Samuel (1959)\n",
    "\n",
    "- \"A machine-learning system is trained rather than explicitly programmed. Itâ€™s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task.\" &mdash; Francois Chollet, _Deep Learning with Python_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Three Types of Learning\n",
    "\n",
    "- Supervised Learning\n",
    "\n",
    "- Unsupervised Learning\n",
    "\n",
    "- Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "- Requires labelled data set (e.g. MC truth)\n",
    "\n",
    "- Direct feedback on model performance while training\n",
    "\n",
    "- Application to two kinds of problems\n",
    "\n",
    "    - Classification -> fixed output types\n",
    "    - Regression -> continuous output\n",
    "\n",
    "<img width=\"500\" alt=\"png\" src=\"images/Learning/NoLabels.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "- No labels on data set (e.g. no MC truth)\n",
    "\n",
    "- No direct feedback while training model\n",
    "\n",
    "- Identify underlying structure in data\n",
    "\n",
    "- Application to two main subfields\n",
    "\n",
    "    - Clustering -> organize data stack into meaningful subgroups\n",
    "    - Dimensionality reduction -> preprocessing of large data sets\n",
    "\n",
    "<img width=\"600\" alt=\"png\" src=\"images/Learning/UnsupervisedLearning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "- Used for training decision making process (e.g. playing chess)\n",
    "\n",
    "- Learn a series of actions by interacting with environment\n",
    "\n",
    "- Requires a reward system to optimize, improve performance\n",
    "\n",
    "<img width=\"600\" alt=\"png\" src=\"images/Learning/ReinforcementLearning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Scheme for Building ML Systems\n",
    "\n",
    "![simple_perceptron](images/Learning/GeneralScheme.png \"general_scheme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "- From labelled data, learn a mapping from input data to desired output\n",
    "\n",
    "- The goal is to generalize well to future, unseen data\n",
    "\n",
    "- Application to two kinds of problems\n",
    "\n",
    "    - Classification -> fixed output types\n",
    "    - Regression -> continuous output\n",
    "\n",
    "<img width=\"500\" alt=\"png\" src=\"images/Learning/NoLabels.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning\n",
    "\n",
    "\n",
    "<img width=\"700\" alt=\"png\" src=\"images/Learning/Labels.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning\n",
    "\n",
    "- Requires labelled data set (e.g. MC truth)\n",
    "\n",
    "- Direct feedback on model performance while training\n",
    "\n",
    "<img width=\"600\" alt=\"png\" src=\"images/Learning/Training.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning\n",
    "\n",
    "- Choose ML algorithm appropriate for problem\n",
    "\n",
    "<img width=\"600\" alt=\"png\" src=\"images/Learning/Algorithm.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Supervised Learning\n",
    "\n",
    "- Application to two kinds of problems\n",
    "\n",
    "    - Classification -> fixed output types\n",
    "    - Regression -> continuous output\n",
    "\n",
    "<img width=\"600\" alt=\"png\" src=\"images/Learning/Prediction.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Basics\n",
    "\n",
    "- Inspired by nature\n",
    "- The Perceptron\n",
    "- Learning from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neuronal Inspiration\n",
    "\n",
    "\n",
    "![overview](images/raschka/perceptron_neuron.png \"neuron\")\n",
    "\n",
    "<p style=\"font-size:14px\">\n",
    "Image source: <a href=\"http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html\">Artificial Neurons and the McCulloch-Pitts Model</a> by Sebastian Raschka\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Towards an Artificial Neuron\n",
    "\n",
    "Desired Components\n",
    "\n",
    "   - Set of input accepting 'dendrites'\n",
    "   - Inner body that 'activates' based on input signals\n",
    "   - Inner function that 'decides' whether to fire\n",
    "   - Output signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Towards an Artificial Neuron\n",
    "\n",
    "Desired Characteristics\n",
    "\n",
    "   - Simple mathematical functions\n",
    "       - Easy to evaluate\n",
    "       - Differentiable\n",
    "       \n",
    "   - Building block\n",
    "       - Single unit\n",
    "       - Can be stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Perceptron\n",
    "\n",
    "\n",
    "Components\n",
    "\n",
    "   - Set of input **features**: $X$\n",
    "   - Set of real valued **weights**: $W$\n",
    "   - **Activation** function: $\\phi$\n",
    "   - **Decision** function\n",
    "   - Output value (binary, $\\mathbb{R}$): $y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    \n",
    "![simple_perceptron](images/Learning/Perceptron/SimplePerceptron.png \"simple_perceptron\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inputs and Weights\n",
    "\n",
    "\n",
    "First take input information $x_i$ and combine with weights $w_i$\n",
    "    \n",
    "    \n",
    "![simple_perceptron_input](images/Learning/Perceptron/SimplePerceptron_Inputs.png \"simple_perceptron_input\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Activation Function\n",
    "\n",
    "\n",
    "Combine information into single variable, $z$\n",
    "\n",
    "$$\n",
    "\\Sigma \\rightarrow z = \\sum_{i=0}^{n} w_i x_i\n",
    "$$\n",
    "\n",
    "so that $\\phi$ can *activate* based on its value.\n",
    "    \n",
    "    \n",
    "![simple_perceptron_act](images/Learning/Perceptron/SimplePerceptron_Activation.png \"simple_perceptron_act\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "As we'll see, it's really nice if $\\phi(z)$ is **continuous** & **differentiable**, and acts **linear** near origin.\n",
    "\n",
    "Common forms of $\\phi(z)$ used to *squish* input into some range (monotonically)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Adaptive Linear Neuron (AdaLine): $z$\n",
    "- Logistic: $\\frac{1}{1+e^{-z}}$\n",
    "- Arctangent: $\\tan^{-1}(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "Common forms of $\\phi(z)$ used to *squish* input into some range (monotonically):\n",
    "\n",
    "- Adaptive Linear Neuron (AdaLine): $z$\n",
    "- Logistic: $\\frac{1}{1+e^{-z}}$\n",
    "- Arctangent: $\\tan^{-1}(z)$\n",
    "\n",
    "![act_func](images/notebook/activation_functions.png \"act_func\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Decision Function\n",
    "\n",
    "\n",
    "Based on activation, make a decision $\\mathcal{D}$ to provide an output $y$.\n",
    "    \n",
    "    \n",
    "![simple_perceptron_dec](images/Learning/Perceptron/SimplePerceptron_Decision.png \"simple_perceptron_dec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Functions\n",
    "\n",
    "Form of $\\mathcal{D}$ is just a threshold function of $\\phi(z)$ and thus of $z$, so:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \n",
    "\\begin{cases}\n",
    "1, & \\text{if } z \\ge b\\\\\n",
    "\\{-1,0\\}, & \\text{if } z < b\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "But since $b$ is some threshold value, we can absorb in our definition of $w$:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=0}^{n} w_i x_i - b = \\sum_{i=0}^{n+1} w_i x_i\n",
    "$$\n",
    "\n",
    "where $w_0 = -b$ and $x_0 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Functions\n",
    "\n",
    "So now we can write $\\mathcal{D}$ as:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \n",
    "\\begin{cases}\n",
    "1, & \\text{if } z \\ge 0\\\\\n",
    "\\{-1,0\\}, & \\text{if } z < 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this example, we are doing binary **classification**, so we can choose either $-1$ or $0$ as the latent state depending on the problem setup.\n",
    "\n",
    "For **regression** for example, we could $\\mathcal{D} = \\phi(z) = \\frac{1}{1+e^{-z}}$ (Logistic), providing class-membership probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Functions (Binary)\n",
    "\n",
    "Forms of $\\mathcal{D}$ are just threshold functions.\n",
    "\n",
    "Does $z$ and thus $\\phi(z)$ reach a value sufficient to *fire*, i.e. $\\mathcal{D}=+1$?\n",
    "\n",
    "![dec_func](images/notebook/decision_functions.png \"dec_func\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Logistic Function\n",
    "\n",
    "A quick aside for the Logistic function.\n",
    "\n",
    "Consider two possible outcomes $y\\in \\{a,b\\}$, with probabilities $\\{p, 1-p\\}$.\n",
    "\n",
    "The ratio $\\frac{p}{1-p} \\in (0,\\infty)$ provides the **odds** for event $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Take the logarithm: $\\log \\frac{p}{1-p} \\in \\mathbb{R}$.\n",
    "\n",
    "Recall that $z = \\sum_i w_i x_i \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Logistic Function\n",
    "\n",
    "Consider two possible outcomes $y\\in \\{a,b\\}$, with probabilities $\\{p, 1-p\\}$.\n",
    "\n",
    "The ratio $\\frac{p}{1-p} \\in (0,\\infty)$ provides the **odds** for event $a$.\n",
    "\n",
    "Take the logarithm: $\\log \\frac{p}{1-p} \\in \\mathbb{R}$.\n",
    "\n",
    "Recall that $z = \\sum_i w_i x_i \\in \\mathbb{R}$.\n",
    "\n",
    "So let's *squish* our *z* with this function:\n",
    "$$\n",
    "\\log \\frac{p}{1-p} = z\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1-p}{p} = e^{-z}\n",
    "$$\n",
    "\n",
    "$$\n",
    "1-p = p \\ e^{-z}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(y=a|\\mathbf{x}) = \\phi(z) = \\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logistic Function\n",
    "\n",
    "$\\phi_{\\text{L}}(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "![log_func](images/notebook/logistic_function.png \"log_func\")\n",
    "\n",
    "Now we can define $\\mathcal{D}$ to provide either probability $p$ or request a binary decision ${0, 1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptron Learning\n",
    "\n",
    "So now we have the mechanics of the perceptron itself.\n",
    "\n",
    "Given inputs $X$, weights $W$, activation and decision functions, we can get an output $y$ \n",
    "\n",
    "So now, how do we train it to **learn** something?\n",
    "\n",
    "![simple_perceptron](images/Learning/Perceptron/SimplePerceptron.png \"simple_perceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptron Learning\n",
    "\n",
    "\n",
    "We need two main things to learn:\n",
    "\n",
    "- Quantify how *good* our perceptron is doing\n",
    "\n",
    "- Ability to *tune* our perceptron based on this performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost Function\n",
    "\n",
    "- Some way of quantifying how *good* our perceptron is doing\n",
    "\n",
    "In **supervised** learning &rarr; **labelled** data sets, $y_{\\text{true}}$.\n",
    "\n",
    "Consider evaluating a __cost function__ $J$:\n",
    "\n",
    "$$\n",
    "J(Y_{\\text{true}}, Y) \\propto \\sum_{\\mu=0}^{M} (y_{\\text{true}}^\\mu - y^\\mu)^2\n",
    "$$\n",
    "\n",
    "over a data set with $M$ samples, where $y^\\mu$ is the perceptron output for sample $\\mu$.\n",
    "\n",
    "Here just the **mean squared error** (MSE).\n",
    "\n",
    "$J$ is a metric we want to **minimize**!\n",
    "\n",
    "In general referred to as an *objective* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## List of Common Objective Functions\n",
    "\n",
    "MSE:\n",
    "$$\n",
    "\\sum_{i=0}^{M} (y_{\\text{true}}^{\\mu} - y^{\\mu})^2\n",
    "$$\n",
    "\n",
    "Binary cross-entropy. Model predicts $p$ while true value is $t$.\n",
    "$$\n",
    "-t \\log (p) - (1-t) \\log(1-p)\n",
    "$$\n",
    "\n",
    "Categorical cross-entropy. Generalization for multiclass logarithmic loss. Target $t_{ij}$, prediction $p_{ij}$.\n",
    "$$\n",
    "-\\sum_{j} t_{ij} \\log( p_{ij})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost Function Minimization\n",
    "\n",
    "Time to minimize!\n",
    "\n",
    "Take a derivative w.r.t. to... ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Only free paramerters are the weights $w_i$. Thus,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} \\bigg\\rvert _{w_{\\text{min}}} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial w_i} &= \\frac{\\partial}{\\partial w_i} \\sum_{\\mu=0}^{M} (y_{\\text{true}}^\\mu - y^\\mu)^2 \\\\\n",
    "&= \\frac{\\partial}{\\partial w_i} \\sum_{\\mu=0}^{M} (y_{\\text{true}}^\\mu - y^{\\mu})^2 \\\\\n",
    "&= \\sum_{\\mu=0}^{M} \\frac{\\partial}{\\partial w_i} (y_{\\text{true}}^\\mu - y^{\\mu})^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cost Function Minimization\n",
    "\n",
    "$$\n",
    "y \\rightarrow \\phi(z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial w_i}\n",
    "&= \\sum_{\\mu=0}^{M} \\frac{\\partial}{\\partial w_i} \\left(y_{\\text{true}}^\\mu - \\phi^{\\mu}(z)\\right) ^2 \\\\\n",
    "&= \\sum_{\\mu=0}^{M} (-2) \\left( y_{\\text{true}}^\\mu - \\phi^{\\mu}(z) \\right) \\frac{\\partial \\phi^{\\mu}(z)}{\\partial w_i} \\\\\n",
    "&= \\sum_{\\mu=0}^{M} (-2) \\left( y_{\\text{true}}^\\mu - \\phi^{\\mu}(z) \\right) \\frac{\\partial \\phi^{\\mu}(z)}{\\partial z}\n",
    "\\frac{\\partial z}{\\partial w_i} \\\\\n",
    "&= \\sum_{\\mu=0}^{M} (-2) \\left( y_{\\text{true}}^\\mu - \\phi^{\\mu}(z) \\right) \\frac{\\partial \\phi^{\\mu}(z)}{\\partial z} x_i\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cost Function Minimization\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} \\bigg\\rvert _{w_{\\text{min}}} = \\sum_{\\mu=0}^{M} \\left( y_{\\text{true}}^\\mu - \\phi^{\\mu}(z) \\right) \\frac{\\partial \\phi^{\\mu}(z)}{\\partial z}\\bigg\\rvert _{w_{\\text{min}}} x_i = 0\n",
    "$$\n",
    "\n",
    "So we have these pieces:\n",
    "$$\n",
    "x_i \\, \\, , \\, \\, \\, \\phi'= \\frac{\\partial \\phi}{\\partial z}\n",
    "$$\n",
    "\n",
    "and we can evaluate this:\n",
    "$$\n",
    "\\text{Error} = \\left( y_{\\text{true}}^\\mu - y^\\mu \\right) = \\left( y_{\\text{true}}^\\mu - \\phi^{\\mu}(z) \\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Woopdie-doo...\n",
    "\n",
    "Looks a little nasty to evaluate.\n",
    "\n",
    "So, what are we going to do with these to find $w_{\\text{min}}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost Function Minimization\n",
    "\n",
    "- Ability to *tune* our perceptron based on this performance\n",
    "\n",
    "We're going to inform our weights via the Error to search for the $w_{\\text{min}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![perceptron_err](images/Learning/Perceptron/Perceptron_Error.png \"perceptron_err\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptron Learning Rule\n",
    "\n",
    "Consider small displacement of a function\n",
    "\n",
    "<table border=\"0|0\">\n",
    "    <td align=\"center\">\n",
    "        For small $\\delta w > 0$, <br><br>\n",
    "\n",
    "        $$\n",
    "        J(w+\\delta w) \\approx J(w) + J'(x) \\ \\delta w\n",
    "        $$ \n",
    "        <br> <br>\n",
    "\n",
    "        so <br><br>\n",
    "\n",
    "        $$\n",
    "        J\\left(w-\\delta w \\ \\mathrm{sgn} \\ J'(w) \\right) \\leq J(w) \\, .\n",
    "        $$\n",
    "        \n",
    "        <br><br>\n",
    "        \n",
    "    </td>\n",
    "    \n",
    "    <td align=\"center\">\n",
    "        <img width=\"500\" alt=\"jpg\" src=\"images/Learning/Gradient_Descent.png\">\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Perceptron Learning Rule\n",
    "\n",
    "So let's update the weights $w$ via something like <br>\n",
    "\n",
    "$$\n",
    "w \\leftarrow w + \\delta w\n",
    "$$ \n",
    "<br>\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\delta w = - \\eta \\ J'(w)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "i.e., follow negative gradient to find minimum of $J(w)$, at a **learning rate** $\\eta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent\n",
    "\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. Initialize $w_i$\n",
    "2. Evaluate perceptron output, $y$\n",
    "3. Calculate $\\frac{\\partial J(w)}{\\partial w_i}$, $\\delta w_i$\n",
    "4. Update weights: $w_i \\leftarrow w_i + \\delta w_i$\n",
    "5. Return to Step 2 if stopping criteria not met.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "What we've seen so far is called **Batch** Gradient Descent.\n",
    "\n",
    "Three main methods:\n",
    "\n",
    "- Batch (BGD)\n",
    "  - $\\rightarrow \\sum_{\\mu=0}^{M}$\n",
    "  - Uses **all** examples!\n",
    "  - Slow, memory requirements..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "- Stochastic (SGD)\n",
    "  - $k \\in M \\rightarrow J_{k}'$\n",
    "  - **One** example! Used for **online** learning (data continuously arriving).\n",
    "  - Potentially noisey path to $J_{\\text{min}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "- Mini-Batch: (MBD)\n",
    "  - $ S \\subset M \\rightarrow \\sum_{\\mu \\in S}$\n",
    "  - **Subset** of examples. \n",
    "  - Compromise, most stable. Typically $|S| = 128, 256, ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning Rate\n",
    "\n",
    "\n",
    "This first <b>hyperparameter</b> $\\eta$ determines how far $\\delta w$ will jump searching for $J_{\\text{min}}$\n",
    "\n",
    "![grad_eta](images/Learning/Gradient_Descent_eta.png \"grad_eta\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning Rate\n",
    "\n",
    "Of course, we are not guaranteed a <b>global</b> minimum \n",
    "via $\\frac{\\partial J}{\\partial w_i} \\bigg\\rvert _{w_{\\text{min}}} = 0$.\n",
    "\n",
    "Consider [deep learning](https://arxiv.org/pdf/1409.4842v1.pdf) networks with $O(>10^6)$ weights!\n",
    "Choose $\\eta$ wisely.\n",
    "\n",
    "![grad_local](images/Learning/Gradient_Descent_Localmin.png \"grad_local\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning Rate - Adaptive\n",
    "\n",
    "One possible solution is to use an adaptive rate.\n",
    "\n",
    "Example: Annealing\n",
    "$$\n",
    "\\eta = \\frac{c_1}{k+c_2}\n",
    "$$\n",
    "\n",
    "where $c_i$ are constants and $k$ is the current iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Iris Dataset\n",
    "\n",
    "- Set of 150 samples (individual flowers) that have 4 features: sepal length, sepal width, petal length, and petal width (all in cm). Collected in 1936 by R. Fisher.\n",
    "    \n",
    "- Each sample is labeled by its species: Iris Setosa, Iris Versicolour, Iris Virginica\n",
    "\n",
    "- Task is to develop a model that predicts iris species\n",
    "    \n",
    "- Dataset freely available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/iris)\n",
    "\n",
    "![Iris dataset](images/iris-nolabel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Iris Learning Example\n",
    "\n",
    "Consider just\n",
    "- Two species: Setosa, Versicolour\n",
    "- Two feature variables: sepal length, petal length\n",
    "\n",
    "![Iris data](images/notebook/simple_iris_data.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Iris Learning Example\n",
    "\n",
    "By eye, we can separate these.\n",
    "\n",
    "Can our perceptron learn a decision boundary for classification?\n",
    "\n",
    "![Iris data](images/notebook/simple_iris_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Iris Learning Example\n",
    "\n",
    "Perceptron:\n",
    "- Two feature inputs (sepal & petal lengths)\n",
    "- Unit Step act. function\n",
    "- Learning rate: $\\eta = 0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we know when the perceptron has learned?\n",
    "\n",
    "Look at evolution of Error and $J(w)$ with training iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Iris Learning Example\n",
    "\n",
    "Perceptron:\n",
    "- Two feature inputs (sepal & petal lengths)\n",
    "- **Unit Step** act. function (binary classification -> **quantized** values)\n",
    "- Learning rate: $\\eta = 0.01$\n",
    "\n",
    "![Unit_01](images/notebook/UnitStep_cost_errors_eta0.100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Iris Learning Example\n",
    "\n",
    "Decision Boundary with **Unit Step**\n",
    "\n",
    "![Unit_dec](images/notebook/UnitStep_decision_boundary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mislabelled Example\n",
    "\n",
    "Linearly separable.\n",
    "\n",
    "What happens if we 'mis-classify' our training set, i.e. no longer linearly separable?\n",
    "\n",
    "![Iris_mis](images/notebook/mislabel_iris_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No convergence for non-linearly separable (mislabelled) data set.\n",
    "\n",
    "![Unit_dec](images/notebook/mislabel_UnitStep_cost_errors_eta0.100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Continuous Activation\n",
    "\n",
    "Perceptron:\n",
    "- Two feature inputs (sepal & petal lengths)\n",
    "- **AdaLine** act. function\n",
    "- Learning rate: $\\eta = 0.01$\n",
    "\n",
    "![Ada_01](images/notebook/AdaLine_cost_errors_eta0.010.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Iris Boundary\n",
    "\n",
    "Decision Boundary with **AdaLine**\n",
    "\n",
    "![Unit_dec](images/notebook/AdaLine_decision_boundary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mislabelled AdaLine\n",
    "\n",
    "Perceptron:\n",
    "- **AdaLine** act. function\n",
    "- Learning rate: $\\eta = 0.01$\n",
    "- **Converges!** Of course with higher errors.\n",
    "\n",
    "![mis_Ada](images/notebook/mislabel_AdaLine_cost_errors_eta0.010.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comparing Learning Rates\n",
    "\n",
    "Perceptron:\n",
    "- **AdaLine** act. function, $\\eta = {0.1,0.001}$\n",
    "- Can have major differences in minimizing $J(w)$.\n",
    "\n",
    "![mis_Ada](images/notebook/AdaLine_cost_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Scaling\n",
    "\n",
    "- Weights typically initialized by $N(0,\\epsilon)$, $\\epsilon$ small\n",
    "- Features may cover large range\n",
    "- Can scale $\\mathbf{x}_i \\rightarrow \\frac{\\mathbf{x}_i - \\mu_i}{\\sigma_i}$\n",
    "    - $\\mu_i$ is mean of feature $i$ over data set\n",
    "    - $\\sigma_i$ is variance of feature $i$ over data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![iris_norm](images/notebook/simple_norm_iris_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Much faster convergence just by preprocessing features!\n",
    "\n",
    "![norm_Ada](images/notebook/AdaLine_norm_cost_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualizing Learning\n",
    "\n",
    "Notebook with some animations.\n",
    "\n",
    "- Visualize learning of boundary\n",
    "    - Different $\\phi(z)$\n",
    "    - Non-scaled and scaled data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Regression\n",
    "\n",
    "- Fit a single Perceptron to one flower type (Setosa)\n",
    "- Now one explanatory variable (Sepal length)\n",
    "\n",
    "![setosa](images/notebook/setosa_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Regression\n",
    "\n",
    "- Smooth convergence of cost minimization\n",
    "- Visual animation of fit\n",
    "\n",
    "![setosa](images/notebook/AdaLine_regression_cost_errors_eta0.040.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further Complications\n",
    "\n",
    "- Our examples are rather simple\n",
    "- Test a more complicated regression $y_{\\text{true}}(x) = cos(\\frac{3\\pi}{2}x)$\n",
    "- Using polynomials of different degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![under_over_fitting](images/underfitting_overfitting_regression.png)\n",
    "\n",
    "<p style=\"font-size:14px\">\n",
    "Image source: <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html\">Underfitting vs. Overfitting</a> scikit-learn.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "- Bias: mean deviation of predictions from true values\n",
    "- Variance: variability of model to classify a sample instance (systematic error)\n",
    "\n",
    "![under_over_fitting](images/underfitting_overfitting_classification.png)\n",
    "\n",
    "<p style=\"font-size:14px\">\n",
    "Image source: <a href=\"http://www.bogotobogo.com/python/scikit-learn/images/NeuralNetwork7-Overfitting/\">Underfitting vs. Overfitting</a> scikit-learn.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "- Can use **regularization** to smooth learning\n",
    "- For example, we don't want weight amplitudes to grow uncontrolled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- L2 Regularization\n",
    "  - $\\lambda||\\mathbf{w}||^2 = \\lambda \\sum_i w_i^2$\n",
    "  - New hyperparameter $\\lambda$\n",
    "  - Include reg. term in minimization procedure\n",
    "  - Feature scaling important to use regularization (on same footing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantifying Performance\n",
    "\n",
    "- Under/over fitting extended to ML\n",
    "    - Hyperparameter effects on performance\n",
    "    - How to quantify quality of trained model\n",
    "    - How do we know we are under/over fitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Splitting up labelled data set\n",
    "    - Training set ($\\sim70\\%$)\n",
    "    - Validation set ($\\sim30\\%$)\n",
    "\n",
    "- Further test performance on entirely unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quantifying Performance\n",
    "\n",
    "- Validation score: a measure of quality\n",
    "    - Classification: fraction of **correct** identifications\n",
    "    - Regression: mean accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Not learn enough, or perfectly trained but can't generalize new data\n",
    "- Model Complexity: polyn, hyperparameters ($\\eta$, $N_{\\text{iter}}$, regularization param)\n",
    "\n",
    "![validation_curve](images/notebook/validation_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to General Scheme for Building ML Systems\n",
    "\n",
    "![simple_perceptron](images/Learning/GeneralScheme.png \"general_scheme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML Algorithms Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Support Vector Machines (SVM)\n",
    "- Decision Trees\n",
    "- Artificial Neural Networks (ANN)\n",
    "  - Multilayer Networks\n",
    "  - Convolutional NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "- Maximize the **margin**: distance between **support vectors**\n",
    "- Support vectors defined by nearest training samples to hyperplane\n",
    "\n",
    "![SVM](images/SVM-Hyperplane-Maximizing-Margin.png)\n",
    "\n",
    "<p style=\"font-size:14px\">\n",
    "Image source: <u>Python Machine Learning</u> by Sebastian Raschka\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "- Maximize the **margin**: distance between **support vectors**\n",
    "- Support vectors defined by nearest training samples to hyperplane\n",
    "\n",
    "$$\n",
    "\\sum_i w_i x^{\\text{upper}}_i = +1 \\\\\n",
    "\\sum_i w_i x^{\\text{lower}}_i = -1 \\\\\n",
    "\\sum_i w_i (x^{\\text{upper}}_i - x^{\\text{lower}}_i) = 2 \\\\\n",
    "$$\n",
    "Normalize:\n",
    "$$\n",
    "\\frac{\\sum_i w_i (x^{\\text{upper}}_i - x^{\\text{lower}}_i)}{||\\mathbf{w}||} = \\frac{2}{||\\mathbf{w}||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "\n",
    "- So maximize $\\frac{2}{||\\mathbf{w}||}$ subject to classification conditions:\n",
    "$$\n",
    "y_k \\sum_i w_i x^k_i \\ge 1 \\text{for } k = {1...N}\n",
    "$$\n",
    "\n",
    "- Actually easier to minimize $||\\mathbf{w}||$ -> quadratic optimization problem\n",
    "\n",
    "- Introduce *soft* margins for non-linearly separable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees\n",
    "\n",
    "- Break down data via series of questions\n",
    "- Split at each node via optimization of **Information Gain**\n",
    "\n",
    "![simple_tree](images/simple_tree.png)\n",
    "\n",
    "<p style=\"font-size:14px\">\n",
    "Image source: <u>Python Machine Learning</u> by Sebastian Raschka\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "- Information gain (IG) as the objective function\n",
    "$$\n",
    "\\text{IG}(D_p,f) = I(D_p) - \\sum_{j=1}^m \\frac{N_j}{N_p}I(D_j)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "   - $f$ is the feature for the split\n",
    "   - $D_p, \\, D_j$ are the data set of the parent and $j$th child node\n",
    "   - $I$ is the impurity measure\n",
    "   - N_p is the # samples at the parent node\n",
    "   - N_j is the # samples in the $j$th child node\n",
    "   \n",
    "IG is the different between the impurity of the parent node and the sum of the child node impurities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "Most libraries implement **binary** splitting:\n",
    "$$\n",
    "\\text{IG}(D_p,f) = I(D_p) - \\frac{N_\\text{left}}{N_p}I(D_\\text{left}) - \\frac{N_\\text{right}}{N_p}I(D_\\text{right})\n",
    "$$\n",
    "\n",
    "Common impurity measures $I(t)$ at node $t$:\n",
    "\n",
    "  - Entropy: $I(t) = - \\sum_{i=1}^c p(i|t) \\log_2 p(i|t)$\n",
    "  - Gini: $I(t) = \\sum_{i=1}^c p(i|t) \\left( 1- p(i|t) \\right) = 1 - \\sum_{i=1}^c p(i|t)^2$\n",
    "  - Classification error: $I(t) = 1- \\text{max} p(i|t)$\n",
    "\n",
    "where $p(i|t)$ is the fraction of samples belonging to class $i$ at node $t$.\n",
    "\n",
    "\n",
    "Can overtrain easily so need **pruning** to limit max. depth of tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "\n",
    "![ann_scheme](images/ANN_Scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Artificial Neural Networks\n",
    "\n",
    "- Built up of many connected artificial neurons\n",
    "- Building block is our little perceptron!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Applications:\n",
    "  - Computer vision\n",
    "  - Speech recognition\n",
    "  - Social network filtering\n",
    "  - Game play\n",
    "  - Physics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Artificial Neural Networks\n",
    "\n",
    "- Single layer network\n",
    "\n",
    "![single](images/ANN_Single_Layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Artificial Neural Networks\n",
    "\n",
    "- Single layer network\n",
    "- Keeping track of indices\n",
    "  - $i$ input features\n",
    "  - $j$ outputs\n",
    "  - $i\\times j$ weights\n",
    "- Optimization with $J\\sim \\sum_{\\mu,k} J\\left( y_{k, \\text{true}}^{\\mu} - y_{k}^{\\mu} \\right)$ ($k$th output and $\\mu$th sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Artificial Neural Networks\n",
    "\n",
    "Adding more layers means adding more indices.\n",
    "\n",
    "Optimization algorithm (backpropagation) remains the same.\n",
    "\n",
    "Just start from the outputs, and move backwards to fine tune the weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![double](images/ANN_Double_Layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ANN\n",
    "\n",
    "- Image Analysis\n",
    "  - B&W: each pixel is a feature (grayscale $[0,255] \\rightarrow [0,1]$ scaled)\n",
    "  - RGB: each pixel provides 3 features (R, G, B $[0,255] \\rightarrow [0,1]$ scaled)\n",
    "  - Each PMT provides charge, Q\n",
    "  \n",
    "- Event Analysis\n",
    "  - Sophisticated reco variables: lateral dist, theta, timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convolutional NN\n",
    "\n",
    "- Primarily used in image analysis\n",
    "  - Convolutions account for neighboring pixels\n",
    "  - Great for identifying sub-features in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MaxPool Convolution\n",
    "\n",
    "![simple_conv](images/Simple_Convolution.png)\n",
    "<p style=\"font-size:14px\">\n",
    "Image source: [Understanding CNN for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/) Denny Britz\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deep Convolutional Network\n",
    "\n",
    "Layers of convolutions, and samplings...\n",
    "\n",
    "![simple_conv](images/DeepConvNet.png)\n",
    "<p style=\"font-size:14px\">\n",
    "Image source: [Introduction to CNN for Vision Tasks](https://pythonmachinelearning.pro/introduction-to-convolutional-neural-networks-for-vision-tasks/) \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Keras Tutorial\n",
    "\n",
    "Following the first few sections of [Deep Learning with Keras](https://www.amazon.com/Deep-Learning-Keras-Implementing-learning/dp/1787128423)\n",
    "\n",
    "- Single Layer\n",
    "- M-Hidden Layers\n",
    "- Dropout in M Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Keras Tutorial\n",
    "\n",
    "- Python API for running [TensorFlow](https://www.tensorflow.org/)\n",
    "- TensorFlow: Google's symolic library for tensor math\n",
    "- Online playground [here](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.21422&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "- Keras more intuitive functionality to develop Deep Learning models\n",
    "  - Building blocks: layers, objective and activation functions, optimizers\n",
    "  - Distributed training of networks via GPUs and GPU clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Installation\n",
    "\n",
    "A few things prior to running scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Install somethings with `pip`\n",
    "\n",
    "  - `pip install numpy scipy scikit-learn pillow h5py`\n",
    "  - `pip install Theano`\n",
    "  - `pip install --upgrade tensorflow`\n",
    "  - `pip install keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now test your installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`import theano\n",
    "import theano.tensor as T\n",
    "x = T.dmatrix('x')\n",
    "s = 1 / (1 + T.exp(-x))\n",
    "logistic = theatno.function([x], s)\n",
    "logistic([[0, 1], [-1. -1]])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First Keras Script\n",
    "\n",
    "Let's go train a single later NN on the MNIST data set: 70k handwritten (labelled) digits.\n",
    "\n",
    "![mnist](images/MnistExamples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## First Keras Script\n",
    "\n",
    "We're going to define:\n",
    "\n",
    "- 10 outputs (10 digits)\n",
    "- Split training and validation set: 80 / 20\n",
    "- Mini-batch sets of 128 samples\n",
    "- Objective function: categorical cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`# Network & training\n",
    "N_EPOCH = 5 #200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "N_CLASSES = 10 # No. outputs = No. digits\n",
    "OPTIMIZER = SGD() # SGD optimizer\n",
    "N_HIDDEN = 128 # No. hidden nodes in layer\n",
    "VALIDATION_SPLIT = 0.2 # fraction of training set used for validation\n",
    "LOSS = 'categorical_crossentropy' #categorical_crossentropy, binary_crossentropy, mse\n",
    "METRIC = 'accuracy' #accuracy, precision, recall\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`# Data -> shuffled and split between training and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()`\n",
    "`# X_train is 60,000 rows of 28x28 values -> to be reshaped to 60,000 x 784\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000,RESHAPED)\n",
    "X_test = X_test.reshape(10000,RESHAPED)`\n",
    "\n",
    "`# Need to make float32 for GPU use\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')`\n",
    "\n",
    "`# Normalize grey-scale values\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "`\n",
    "`\n",
    "print(X_train.shape[0], ' training samples')\n",
    "print(X_test.shape[0], ' testing samples')\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`# Convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, N_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, N_CLASSES)`\n",
    "\n",
    "`# N_CLASSES outputs, final stage is normalized via softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "`\n",
    "\n",
    "`# Compile the model\n",
    "model.compile(loss=LOSS,optimizer=OPTIMIZER, metrics=[METRIC])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`# Train the model\n",
    "history = model.fit(X_train, Y_train, \\\n",
    "                    batch_size=BATCH_SIZE,\\\n",
    "                    epochs=N_EPOCH,\\\n",
    "                    verbose=VERBOSE,\\\n",
    "                    validation_split=VALIDATION_SPLIT)\n",
    "`\n",
    "\n",
    "`# Validation of the model with test set\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score: \", score[0])\n",
    "print(\"Test accuracy: \", score[1])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiple Hidden Layers\n",
    "\n",
    "To add more hidden layers, add the following after the first input layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`# 2nd layer\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu')) # Can use other activation functions here`\n",
    "`# 3rd layer\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu')) # Can use other activation functions here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We also don't need so many iterations, so can try N_EPOCH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- _Python Machine Learning_ by Sebastian Raschka [[GitHub](https://github.com/rasbt/python-machine-learning-book-2nd-edition)][[Amazon](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939)]\n",
    "\n",
    "- _Data Science Handbook_ by Jake VanderPlas [[GitHub](https://github.com/jakevdp/PythonDataScienceHandbook)][[Amazon](https://www.amazon.com/_/dp/1491912057?tag=oreilly20-20)]\n",
    "\n",
    "- _The Elements of Statistical Learning_ by Hastie, Tibshirani and Friedman [[Free book!](https://web.stanford.edu/~hastie/ElemStatLearn/)]\n",
    "\n",
    "- _Deep Learning_ by Ian Goodfellow, Yoshua Bengio, and Aaron Courville [[Amazon](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank you for your attention!\n",
    "\n",
    "![final](images/sunset-gsl.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
